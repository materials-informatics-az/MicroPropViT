# Learning microstructureâ€“property relationships in materials with robust features from vision transformers

Machine learning of microstructure--property relationships from data is an emerging approach in computational materials science. Most existing machine learning efforts focus on the development of task-specific models for each microstructure--property relationship. We propose utilizing a pre-trained foundational vision model for the extraction of task-agnostic microstructure features and subsequent light-weight machine learning. We demonstrate our approach with pre-trained vision transformer models, namely CLIP, DINOV2, and SAM, on two case studies: simulated two-phase microstructures to learn elastic stiffness and experimental data of Ni-base and Co-base superalloys to learn Vickers hardness. Our results show the potential of foundational vision models for robust microstructure representation and efficient machine learning of microstructure--property relationships without the need for expensive task-specific training or fine-tuning.

To test on your own dataset, check out the "Feature_Extraction_ViTs" Notebook under either case study to first extract features from your images using ViTs. 
